{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3bca61",
   "metadata": {},
   "source": [
    "# Overfitting and underfitting\n",
    "\n",
    "In the first lecture, there was a question about optimizing with respect to the training data.\n",
    "Remember that one of the central goals of machine learning is to generalize to unseen data.\n",
    "How are these two notions related?\n",
    "\n",
    "Let's take a step back and reconsider the classification and regression task we have seen so far.\n",
    "In both cases we are persented with a finite dataset consisting of samples and labels.\n",
    "We might fix a performance measure, for example for $k$NN this was given by measuring the percentage of correct answers and for linear regression this was given by the mean squared error.\n",
    "\n",
    "Then we split our dataset into training set and test set and train our model on the training set and train our model on the training set. In case of $k$NN, this was done by simply storing the data so that we have access to it.\n",
    "In case of linear regression, training consists of finding the minimum of the loss/cost function on the training set.\n",
    "This results in a training error.\n",
    "\n",
    "Then we measure performance on the test set which results in a test error.\n",
    "Now the test error will in general be greater or equal to the training error and the goal is to find models so that the gap between them is small (and the errors are small).\n",
    "\n",
    "Now our model is **overfitted**, if the training error is small but the test error is large.\n",
    "If both errors are large then we call the model **underfitted**.\n",
    "\n",
    "There are several possibilities to counter this.\n",
    "* If our model has parameters, we might try and change them. $k$NN does have parameters (the number of neighbors), while linear regression does not have any.\n",
    "* Change the hypothesis space, i.e. change the space of functions we consider. For example only consider polynomials up to a certain degree.\n",
    "* Change the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8243c0",
   "metadata": {},
   "source": [
    "# Example using $k$NN\n",
    "\n",
    "The only parameter we can change in the $k$NN-algorithm is the number of neighbours.\n",
    "Let's see how the number of neighbours affects how well we are able to predict an outcome or how well we are able to generalize.\n",
    "\n",
    "What should be clear is that using only 1 neighbor will fit perfectly the training data.\n",
    "However, we might not get optimal results for prediction.\n",
    "This would be a classical case of overfitting (although this might still be good enough as the Iris flower example shows).\n",
    "On the other hand, using larger number of neighbours somehow simplifies our model.\n",
    "In the extreme case of using the same size as our dataset results in just taking the average of information available.\n",
    "In this case the model would clearly be underfitted.\n",
    "Let's use an example to illustrate this.\n",
    "\n",
    "This time we use the Wisconsin Breast Cancer data set, which records clinical measurements of breast cancer tumors. Each tumor is labeled *benign* or *malignant* and the task is to predict whether a tumor is malignant based on the measurements of the tissue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8de802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "print(cancer.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd67e3b4",
   "metadata": {},
   "source": [
    "The `data` value contains the measurements, let's look at its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f3c9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "print(cancer.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256440a",
   "metadata": {},
   "source": [
    "So we have 569 data points with 30 features each.\n",
    "The features this time are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a0be19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "print(cancer.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a777a09a",
   "metadata": {},
   "source": [
    "You can read more about this dataset by printing out the `DESCR` value if you like.\n",
    "Now let's see how the $k$NN algorithm performs using this dataset.\n",
    "We first split into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2aa6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, \n",
    "                                                   stratify=cancer.target, random_state=66)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef050e",
   "metadata": {},
   "source": [
    "Now we want to plot how well the the algorithm performs with respect to the number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0fe758",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "neighbors_settings = range(1,560)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for k in neighbors_settings:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855aa9c3",
   "metadata": {},
   "source": [
    "Let's plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bdb1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(neighbors_settings, training_accuracy, label='training accuracy')\n",
    "plt.plot(neighbors_settings, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcfff9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
